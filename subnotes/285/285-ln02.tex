\chapter{Supervised Learning of Behaviors}
Chapter Description.

\section{Terminology and Notation in DRL}
In this section, we will cover several temrinologies and notations that the community uses regarding learning situations.

In our paradigm, we concern an input, a system that processes the input, and the output.
Decision-making problems consider these as respectively observations $o_t$, policy $\pi_\theta (a_t|o_t)$, and actions $a_t$.
These symbols are subscripted by time because the context of a decision-making problem is usually a chronological sequence of events.
Note that, the production of next observation, $o_{t+1}$, is based on the impacted state $o_t$ upon the transpiration of $a_t$.
Note that the format of $a_t$, for example, is not limited to a vector; it can also be a continuous distribution, as we would sometimes like to sample actions to take rather than using an almost deterministic policy.
The policy $\pi_\theta$ is parameterized by $\theta$, which is a vector of parameters that the policy uses to make decisions (that is, to provide action provided observation).
They, therefore, assign the probability to all possible actions provided a specific state.

We also introduce the notion of a state, $s_t$, which is a partial observation.
This notion is introduced by the fact that the entirety of an environment is not always observable (and in most situations, we do not observe the entirety of an environment).
Therefore, realistically, the policy we learn is $\pi_\theta (a_t|s_t)$, which we call a partially observed policy.
A very appropriate analogy is perhaps our visual-neural system: our eyes guide our decisions based on what objects are posed in our environment, particularly what is in front of our eyes.
But, we do not gain information from our eyes regarding what is behind us, simply because the environment we are situated in is partially observable: the sensors we have (eyes) simply do not detect what is behind.
Therefore, in reinforcement learning paradigms, we work with state-action pairs rather than observation-action pairs.

We develop this into what we call a Markov Decision Paradigm.
In this paradigm, we assume that the state $s_t$ is sufficient to make decisions, and that the future state $s_{t+1}$ is independent of the past states $s_{t-1}, s_{t-2}, \ldots$ given the current state $s_t$ and the action $a_t$.
The paradigm per se contains Markov-ness; that is, $s_t$ only depends on $s_{t-1}$, and the connection of states is only nonzero for $p(s_{t+1} | s_t, a_t)$.
The involvement of only states and actions is a reflection of the partial observability we suffer in environments, which produces a Partially Observable Markov Decision Process (POMDP).

Note: in some literature, states will be issued as $x_t$, and action $u_t$ instead, due to the involvement of background in contorl theory from some influential figures.

\section{Imitation Learning}
\textbf{Imitation learning} concerns the learning of a policy $\pi_\theta$ from a dataset of expert demonstrations.
\textbf{Behavioral cloning}, an approach of imitation learning, is the idea that, via supervised learning, we learn a policy that regresses an underlying function $f: \mathcal{S} \rightarrow \mathcal{A}$ that maps the expert-induced states to experr-performed actions.
These expert demonstrations are, as assumed, provided to the algorithm.
However, behavioral cloning is usually not a good solution to general problems.
Therefore, we expect the agent to practically clone the behavior (state-action reaction) of the expert.

The reason why is because, although expert demonstrations provide us many trajectories, once we receive a state outside of the provided trajectories, the actions our agent provides are not well-defined in nature.
It is moreso that, once the agent is exposed to an unseen state, the action it provides via its policy does not guarantee a continued cloning of the expert-demonstrated trajectory.
This is because the nature of a trajectory destructs the i.i.d. assumption of supervised learning; that is, because of temporal dependencies, we encounter problems in behavioral cloning that does not appear in conventional supervised learning problems.
(A spam classifier generalizes well to unseen emails, but a behavioral cloning agent does not generalize well to unseen states).
We can propose ad-hoc solutions, such as data augmentation that increases familarity of the agent to unseen states, but these solutions are not always guaranteed to work.

To make behavioral cloning work, we must make modifications to the existing paradigm.
Let us begin with lessons of the story:
\begin{enumerate}
    \item Different from conventional supervised studying problems, imitation learning via behavioral cloning is not guaranteed to work.
    \item To work against it, we can use data augmentation as well as modify data collection methods.
    \item An exotic solution, like a multi-task learning formulation, may help to generalize to unseen states and perform good imitation learning.
    \item The most intuitive approach is perhaps modifying the algorithm along which we do imitation learning.
\end{enumerate}

\section{Theoretical Analysis of Failure in Behavioral Cloning}
The main culprit of behavioral cloning's failure is the dsitributional shift problem.
Suppose that we have some policy $\pi_\theta (a_t | o_t)$ trained on a dataset of expert demonstrations.
Here, let us say that the distribution provided by expert dataset is $p_{data}(o(t))$, but the distribution of observation the policy faces is $p_{\pi_\theta}(o_t)$.
Note that, since our policy is trained under $p_{data} o(t)$, the objective of that training is $\max_\theta \mathbb{E}_{o_t \sim p_{data}(o(t))} \log \pi_\theta (a_t | o_t)$.
However, the policy is evaluated under $p_{\pi_\theta}(o_t)$, which is not the same as the training distribution.
This problem is otherwise known as \textbf{distributional shift}, and this occurs due to the policy's own deviation from the training distribution.

The lesson of such problem is that we should perhaps define more precisely what we want to define as ``well-learned''.
A policy that is good would perhaps not be a point-estimate for actions, since it can easily lead to deviations in policy behavior.
Perhaps we may define a cost otherwise. Suppose that $\pi^*$ is the optimal policy for us to clone:
\[
    c(s_t, a_t) = \begin{cases}
        0 & \text{if } a_t = \pi^* (s_t) \\
        1 & \text{otherwise}
    \end{cases}
\]
Now, then, our training objective becomes:
\[
    \min \mathbb{E}_{s_t \sim p_{\pi_\theta} (s_t)} [c(s_t, \pi_\theta (s_t))]
\]
Assume that we have an upper bound of the policy mistake, $\pi_\theta(a \neq \pi*(s) | s) \leq \epsilon$.
Then, we observe an incurred cost of:
\begin{align*}
    \mathbb{E} &[\sum_t c(s_t, \pi_\theta (s_t))] \\
    &\leq \epsilon T + (1 - \epsilon) (\epsilon (T - 1) + (1 - \epsilon) (\dots)) \in 0(\epsilon T^2)
\end{align*}
Therefore, the error of behavioral cloning is quadratic to the length of its trajectory ($T$).

It turns out that an analysis composed by \href{https://arxiv.org/abs/1011.0686}{Ross et al. (2011)} shows that the error of behavioral cloning is quadratic to the length of the trajectory.
That is, imitation learning is prone to failure at any timestep, and lacks a means of recovery from a policy's small failures.

\section{Addressing Problem of Imitation Learning}
To address the problem of imitation learning, we can consider the following approaches:
\begin{enumerate}
    \item Data Augmentation and collection
    \item Powerful models that make very few mistakes
    \item Multi-task learning formulation of imitation learning
    \item Changing the algorithm of use (where we discuss DAgger)
\end{enumerate}

\textbf{\textit{Data Augmentation and Collection.}}
Behavioral cloning is difficult because the model doesn't generalize well to mistakes.
What is we involve data regarding mistakes instead? If the dataset involves mistakes, due to additional steps in data collection and augmentation process, although the training set will be diluted, we can now access corrections in our BC paradigm.

\textbf{\textit{Powerful Models.}}
Failures from fitting the expert can stem from several reasons.
First, the expert's behavior is non-Markovian. While the policy we train is Markovian-ly conditioned, the expert's behavior are not formulated on a Markovian approach. That is, provided exposure to states $s_t = s_{t'}$, it is likely that the action elicited from these states are different.
Therefore, human demonstrators post a very unnatural circumstance for a Markovian policy.
Perhaps one remedy is to use the entire history of a trajectory, and a sequence model is capable of processing it as a temporal sequence of frames.
However, the exploitation of entire history may still work poorly, simply because including the history may still be harmed by incomplete information, which can lead to causal confusion.
Second, the expert's behavior may be multimodal. That is, the expert may have multiple ways of solving a problem, and the policy we train may not be able to capture all of these modes.
This is specifically unhelpful for a continuous distribution of actions, which rely on the use of mean and variance that is difficultly characterizing for bimodal distributions.
Although we can instead choose expressive continuous distributions for actions (namely, use other classes of distributions), or use an autoregressive discretization with high-dimensional action spaces, they do pose higher computational costs to this procedure.

\textbf{\textit{Multi-task Learning Formulation.}}
Training a policy that has one sole destination of trajectory can be difficult, but perhaps a multi-task learning formulation that attempts to let policies reach multiple different destinations.
This approach can be summarized as ``goal-conditioned behavioral cloning'', which can provide more opportunities to learn corrections despite distributional shift; that is, to maximize $\log \pi_\theta (a_t^i | s_t^i, g = s_T^i)$.
However, this approach actually introduces a secondary source for distributional shift, making the approach theoretically worse.

\textbf{\textit{Changing the Algorithm of Use: DAgger.}}
The idea of DAgger, Dataset Aggregation, is to make $p_{data}(o_t) = p_{\pi_\theta}(o_t)$.
That is, we collect training data from $p_{\pi_\theta}(o_t)$ by running the policy $\pi_\theta (a_t | o_t)$ in the environment.
Then, we aggregate the dataset of expert demonstrations and the dataset of the policy's own data, and train the policy on the aggregated dataset.
The algorithm is as follows:
\begin{enumerate}
    \item Collect a dataset of expert demonstrations $\mathcal{D} = {\{(o_t^i, a_t^i)\}}_{i=1}^N$.
    \item For $k = 1, 2, \ldots, K$:
    \begin{enumerate}
        \item Train the policy $\pi_\theta$ on $\mathcal{D}$.
        \item Collect a dataset of the policy's own data $\mathcal{D}_{\pi_\theta} ={ \{(o_t^i, \pi_\theta (o_t^i))\}}_{i=1}^N$.
        \item Aggregate the datasets $\mathcal{D} \leftarrow \mathcal{D} \cup \mathcal{D}_{\pi_\theta}$.
    \end{enumerate}
\end{enumerate}

\section{A Hint at the Need of Reward and Cost Signals}
Deep learning works best when data is plentiful, but humans are finite sources of data (that is, humans cannot provide all data and label all data).
Therefore, to resolve the disruptive demand of large ground truths, we expect our reinforcement learning algorithm to learn autonomously.
To enable algorithms to evaluate their experiences, and exceeding their own performances, we offer a reward signal called \textbf{reward function}, $r(s, a)$, which provides a numeric evaluation for an observed pair of state and action.
For example, for imitation learning, we can propose the reward function $r(s, a) = \log p(a = \pi^*(s) | s)$.
